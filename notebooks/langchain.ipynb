{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model object - LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Hello, yes! I am chatGPT, an AI-powered chatbot. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "# you can pass any parmaeters of openai Chat APi to LLM constructor\n",
    "llm = OpenAI(model_name = \"gpt-3.5-turbo\",temperature=0.9)\n",
    "\n",
    "prompt = \"Hello there, are you chatGPT?\"\n",
    "\n",
    "# llm is a callable\n",
    "completion = llm(prompt)\n",
    "\n",
    "print(completion)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['product'], output_parser=None, partial_variables={}, template='What is a good name for a company that makes {product}?', template_format='f-string', validate_template=True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# crete an PromptTemplate object\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"What is a good name for a company that makes {product}?\",\n",
    ")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is a good name for a company that makes colorful socks?\n"
     ]
    }
   ],
   "source": [
    "# just a simple string formatting.\n",
    "print(prompt.format(product=\"colorful socks\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine LLM and prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nRainbow Socks Co.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = OpenAI(model_name = \"gpt-3.5-turbo\",temperature=0.9)\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"What is a good name for a company that makes {product}?\",\n",
    ")\n",
    "\n",
    "# an langchain takes in a LLM model and a prompt template\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# you can format template here\n",
    "# and you got an answer from OPENAI\n",
    "chain.run(\"colorful socks\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversational Memory\n",
    "\n",
    "One of the simpler forms of memory occurs in chatbots, where they remember previous conversations. There are a few different ways to accomplish this:     \n",
    "\n",
    "- Buffer: This is just passing in the past N interactions in as context. N can be chosen based on a fixed number, the length of the interactions, or other!\n",
    "- Summary: This involves summarizing previous conversations and passing that summary in, instead of the raw dialogue itself. Compared to Buffer, this compresses information: meaning it is more lossy, but also less likely to run into context length limits.\n",
    "- Combination: A combination of the above two approaches, where you compute a summary but also pass in some previous interactions directly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: hi\\nAI: whats up'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# return the whole hsitory as a string\n",
    "memory = ConversationBufferMemory()\n",
    "memory.save_context({\"input\": \"hi\"}, {\"ouput\": \"whats up\"})\n",
    "memory.load_memory_variables({})\n",
    "\n",
    "# return chat history as a list of objects, which looks like\n",
    "\"\"\"\n",
    "{\n",
    "    'history': [\n",
    "    HumanMessage(content='Hi there!', additional_kwargs={}),\n",
    "    AIMessage(content='Hello! How can I assist you today?', additional_kwargs={}),\n",
    "    HumanMessage(content=\"I'm doing well! Just having a conversation with an AI.\", additional_kwargs={}),\n",
    "    AIMessage(content=\"That's great to hear! I'm always happy to chat with humans. Is there anything specific you'd like to talk about or ask me?\", additional_kwargs={}),\n",
    "    HumanMessage(content='Tell me about yourself.', additional_kwargs={})\n",
    "    ]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# during chatting, the list history is automatically built.\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "memory.save_context({\"input\": \"hi\"}, {\"ouput\": \"whats up\"})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='hi', additional_kwargs={}),\n",
       "  AIMessage(content='whats up', additional_kwargs={})]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "memory.save_context({\"input\": \"hi\"}, {\"ouput\": \"whats up\"})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model_name = \"gpt-3.5-turbo\",temperature=0)\n",
    "memory = ConversationBufferMemory(return_messages = True)\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    verbose=True, \n",
    "    memory= memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[]\n",
      "Human: Hi there!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi there!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content='Hi there!', additional_kwargs={}), AIMessage(content='Hello! How can I assist you today?', additional_kwargs={})]\n",
      "Human: I'm doing well! Just having a conversation with an AI.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"That's great to hear! I'm always happy to chat with humans. Is there anything specific you'd like to talk about or ask me?\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content='Hi there!', additional_kwargs={}), AIMessage(content='Hello! How can I assist you today?', additional_kwargs={}), HumanMessage(content=\"I'm doing well! Just having a conversation with an AI.\", additional_kwargs={}), AIMessage(content=\"That's great to hear! I'm always happy to chat with humans. Is there anything specific you'd like to talk about or ask me?\", additional_kwargs={})]\n",
      "Human: Tell me about yourself.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Sure! I am an AI language model designed to assist humans with various tasks and provide helpful information. I was created by a team of developers who used advanced machine learning algorithms to train me on a vast amount of data. I am constantly learning and improving my abilities to better serve my users. Is there anything else you'd like to know about me?\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Tell me about yourself.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi there!', additional_kwargs={}),\n",
       "  AIMessage(content='Hello! How can I assist you today?', additional_kwargs={}),\n",
       "  HumanMessage(content=\"I'm doing well! Just having a conversation with an AI.\", additional_kwargs={}),\n",
       "  AIMessage(content=\"That's great to hear! I'm always happy to chat with humans. Is there anything specific you'd like to talk about or ask me?\", additional_kwargs={}),\n",
       "  HumanMessage(content='Tell me about yourself.', additional_kwargs={}),\n",
       "  AIMessage(content=\"Sure! I am an AI language model designed to assist humans with various tasks and provide helpful information. I was created by a team of developers who used advanced machine learning algorithms to train me on a vast amount of data. I am constantly learning and improving my abilities to better serve my users. Is there anything else you'd like to know about me?\", additional_kwargs={})]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: haha\\nAI: that's it\"}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory( k=1)\n",
    "memory.save_context({\"input\": \"hi\"}, {\"ouput\": \"whats up\"})\n",
    "memory.save_context({\"input\": \"not much you\"}, {\"ouput\": \"not much\"})\n",
    "memory.save_context({\"input\": \"haha\"}, {\"output\":\"that's it\"})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: not much you\\nAI: not much\\nHuman: haha\\nAI: that's it\"}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationBufferWindowMemory( k=2)\n",
    "memory.save_context({\"input\": \"hi\"}, {\"ouput\": \"whats up\"})\n",
    "memory.save_context({\"input\": \"not much you\"}, {\"ouput\": \"not much\"})\n",
    "memory.save_context({\"input\": \"haha\"}, {\"output\":\"that's it\"})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: hi\\nAI: whats up\\nHuman: not much you\\nAI: not much\\nHuman: haha\\nAI: that's it\"}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationBufferWindowMemory( k=3)\n",
    "memory.save_context({\"input\": \"hi\"}, {\"ouput\": \"whats up\"})\n",
    "memory.save_context({\"input\": \"not much you\"}, {\"ouput\": \"not much\"})\n",
    "memory.save_context({\"input\": \"haha\"}, {\"output\":\"that's it\"})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
